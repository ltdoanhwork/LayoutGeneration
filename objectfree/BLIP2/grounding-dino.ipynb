{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-27T14:48:58.578859Z",
     "iopub.status.busy": "2025-09-27T14:48:58.578592Z",
     "iopub.status.idle": "2025-09-27T14:48:58.583549Z",
     "shell.execute_reply": "2025-09-27T14:48:58.582822Z",
     "shell.execute_reply.started": "2025-09-27T14:48:58.578840Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T14:50:32.933494Z",
     "iopub.status.busy": "2025-09-27T14:50:32.932748Z",
     "iopub.status.idle": "2025-09-27T14:50:33.154535Z",
     "shell.execute_reply": "2025-09-27T14:50:33.153883Z",
     "shell.execute_reply.started": "2025-09-27T14:50:32.933469Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Sep 27 14:50:32 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   44C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   44C    P8             10W /   70W |       1MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grounding DINO Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T14:50:36.431658Z",
     "iopub.status.busy": "2025-09-27T14:50:36.431346Z",
     "iopub.status.idle": "2025-09-27T14:51:15.197734Z",
     "shell.execute_reply": "2025-09-27T14:51:15.196866Z",
     "shell.execute_reply.started": "2025-09-27T14:50:36.431627Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers.git\n",
      "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-z_vcyvei\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-z_vcyvei\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 071eb5334f5a9ac2c7a13515219be8a272388ec6\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.57.0.dev0) (3.18.0)\n",
      "Collecting huggingface-hub==1.0.0.rc1 (from transformers==4.57.0.dev0)\n",
      "  Downloading huggingface_hub-1.0.0rc1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.57.0.dev0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.57.0.dev0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.57.0.dev0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.57.0.dev0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.57.0.dev0) (2.32.4)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers==4.57.0.dev0)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.57.0.dev0) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.57.0.dev0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==1.0.0.rc1->transformers==4.57.0.dev0) (2025.5.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==1.0.0.rc1->transformers==4.57.0.dev0) (0.28.1)\n",
      "Collecting typer-slim (from huggingface-hub==1.0.0.rc1->transformers==4.57.0.dev0)\n",
      "  Downloading typer_slim-0.19.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==1.0.0.rc1->transformers==4.57.0.dev0) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==1.0.0.rc1->transformers==4.57.0.dev0) (1.1.5)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.57.0.dev0) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.57.0.dev0) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.57.0.dev0) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.57.0.dev0) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.57.0.dev0) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.57.0.dev0) (2.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.57.0.dev0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.57.0.dev0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.57.0.dev0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.57.0.dev0) (2025.6.15)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub==1.0.0.rc1->transformers==4.57.0.dev0) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub==1.0.0.rc1->transformers==4.57.0.dev0) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub==1.0.0.rc1->transformers==4.57.0.dev0) (0.16.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.57.0.dev0) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.57.0.dev0) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.57.0.dev0) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.57.0.dev0) (2024.2.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer-slim->huggingface-hub==1.0.0.rc1->transformers==4.57.0.dev0) (8.2.1)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.57.0.dev0) (2024.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub==1.0.0.rc1->transformers==4.57.0.dev0) (1.3.1)\n",
      "Downloading huggingface_hub-1.0.0rc1-py3-none-any.whl (526 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m526.7/526.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typer_slim-0.19.2-py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for transformers: filename=transformers-4.57.0.dev0-py3-none-any.whl size=11510125 sha256=7bde6c860a01c6616adfcf5f244272719f9ebe44280dd340263ab71a6caaeddc\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-65ojudfe/wheels/32/4b/78/f195c684dd3a9ed21f3b39fe8f85b48df7918581b6437be143\n",
      "Successfully built transformers\n",
      "Installing collected packages: typer-slim, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.33.1\n",
      "    Uninstalling huggingface-hub-0.33.1:\n",
      "      Successfully uninstalled huggingface-hub-0.33.1\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.2\n",
      "    Uninstalling tokenizers-0.21.2:\n",
      "      Successfully uninstalled tokenizers-0.21.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.52.4\n",
      "    Uninstalling transformers-4.52.4:\n",
      "      Successfully uninstalled transformers-4.52.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface-hub-1.0.0rc1 tokenizers-0.22.1 transformers-4.57.0.dev0 typer-slim-0.19.2\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample BLIP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T14:57:17.279963Z",
     "iopub.status.busy": "2025-09-27T14:57:17.279252Z",
     "iopub.status.idle": "2025-09-27T14:57:48.357717Z",
     "shell.execute_reply": "2025-09-27T14:57:48.357011Z",
     "shell.execute_reply.started": "2025-09-27T14:57:17.279931Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26df54a869ef4063b23ef8ebda5db50c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/445 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30780a98b3db4d73bc860aa244ff9660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/527 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51323ed141ee4d27bbac3c52f1b420f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b80b799781d48b2ad1fe06f4fa39669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07c7d8cc90934031a95fbb389bba1250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e796ed981f429cad1e9b6a2d5b546f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f80e00a17647ca930ebb85cb1f617d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption BLIP: a close up of a person riding a horse with a sword\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "def generate_caption(image_path, max_new_tokens=25, device=None):\n",
    "    \"\"\"\n",
    "    Sinh caption t·ª´ ·∫£nh b·∫±ng BLIP\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Load model + processor\n",
    "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "    model = BlipForConditionalGeneration.from_pretrained(\n",
    "        \"Salesforce/blip-image-captioning-large\"\n",
    "    ).to(device).eval()\n",
    "\n",
    "    # Load ·∫£nh\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # T·∫°o input cho BLIP\n",
    "    inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Sinh caption\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "# ------------------ DEMO ------------------\n",
    "image_path = \"/kaggle/input/crop-img/Screenshot 2025-09-27 215322.png\"\n",
    "caption = generate_caption(image_path)\n",
    "print(\"Caption BLIP:\", caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T15:05:21.729126Z",
     "iopub.status.busy": "2025-09-27T15:05:21.728553Z",
     "iopub.status.idle": "2025-09-27T15:05:24.213792Z",
     "shell.execute_reply": "2025-09-27T15:05:24.213003Z",
     "shell.execute_reply.started": "2025-09-27T15:05:21.729102Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption BLIP: anime scene of a man on a horse with a crown on his head\n"
     ]
    }
   ],
   "source": [
    "image_path = \"/kaggle/input/crop-sth/0004_clip18_frame094_14653_18.jpg\"\n",
    "caption = generate_caption(image_path)\n",
    "print(\"Caption BLIP:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T14:59:28.697234Z",
     "iopub.status.busy": "2025-09-27T14:59:28.696480Z",
     "iopub.status.idle": "2025-09-27T14:59:31.064941Z",
     "shell.execute_reply": "2025-09-27T14:59:31.064289Z",
     "shell.execute_reply.started": "2025-09-27T14:59:28.697208Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption BLIP: anime character with red hair holding a sword in a cloudy sky\n"
     ]
    }
   ],
   "source": [
    "image_path = \"/kaggle/input/crop-sth/04_l.png\"\n",
    "caption = generate_caption(image_path)\n",
    "print(\"Caption BLIP:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T15:06:09.870380Z",
     "iopub.status.busy": "2025-09-27T15:06:09.869867Z",
     "iopub.status.idle": "2025-09-27T15:06:12.205418Z",
     "shell.execute_reply": "2025-09-27T15:06:12.204772Z",
     "shell.execute_reply.started": "2025-09-27T15:06:09.870357Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption BLIP: a close up of a person on a horse with a sword\n"
     ]
    }
   ],
   "source": [
    "image_path = \"/kaggle/input/crop-sth/04_m.png\"\n",
    "caption = generate_caption(image_path)\n",
    "print(\"Caption BLIP:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T15:06:14.627384Z",
     "iopub.status.busy": "2025-09-27T15:06:14.626711Z",
     "iopub.status.idle": "2025-09-27T15:06:16.957726Z",
     "shell.execute_reply": "2025-09-27T15:06:16.957077Z",
     "shell.execute_reply.started": "2025-09-27T15:06:14.627361Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption BLIP: a close up of a person riding a horse with a sword\n"
     ]
    }
   ],
   "source": [
    "image_path = \"/kaggle/input/crop-sth/04_m_l.png\"\n",
    "caption = generate_caption(image_path)\n",
    "print(\"Caption BLIP:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T15:09:03.570325Z",
     "iopub.status.busy": "2025-09-27T15:09:03.569791Z",
     "iopub.status.idle": "2025-09-27T15:09:05.874003Z",
     "shell.execute_reply": "2025-09-27T15:09:05.873413Z",
     "shell.execute_reply.started": "2025-09-27T15:09:03.570302Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption BLIP: anime image of a woman riding a horse with a sword\n"
     ]
    }
   ],
   "source": [
    "image_path = \"/kaggle/input/crop-sth/0008_clip34_frame083_14653_34.jpg\"\n",
    "caption = generate_caption(image_path)\n",
    "print(\"Caption BLIP:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T15:09:29.412679Z",
     "iopub.status.busy": "2025-09-27T15:09:29.411898Z",
     "iopub.status.idle": "2025-09-27T15:09:31.772619Z",
     "shell.execute_reply": "2025-09-27T15:09:31.771920Z",
     "shell.execute_reply.started": "2025-09-27T15:09:29.412653Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption BLIP: anime image of a man riding a horse with a sword\n"
     ]
    }
   ],
   "source": [
    "image_path = \"/kaggle/input/crop-sth/08.png\"\n",
    "caption = generate_caption(image_path)\n",
    "print(\"Caption BLIP:\", caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### openmmlab-community/mm_grounding_dino_base_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T13:19:14.059685Z",
     "iopub.status.busy": "2025-08-24T13:19:14.058984Z",
     "iopub.status.idle": "2025-08-24T13:19:16.944759Z",
     "shell.execute_reply": "2025-08-24T13:19:16.943870Z",
     "shell.execute_reply.started": "2025-08-24T13:19:14.059661Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "\n",
    "model_id = \"openmmlab-community/mm_grounding_dino_large_all\"\n",
    "device = \"cuda\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T11:28:46.438499Z",
     "iopub.status.busy": "2025-08-24T11:28:46.438274Z",
     "iopub.status.idle": "2025-08-24T11:28:50.276493Z",
     "shell.execute_reply": "2025-08-24T11:28:50.275376Z",
     "shell.execute_reply.started": "2025-08-24T11:28:46.438480Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T13:46:51.810691Z",
     "iopub.status.busy": "2025-08-24T13:46:51.809984Z",
     "iopub.status.idle": "2025-08-24T13:46:56.756111Z",
     "shell.execute_reply": "2025-08-24T13:46:56.755337Z",
     "shell.execute_reply.started": "2025-08-24T13:46:51.810666Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Check for cats and remote controls\n",
    "import re, numpy as np, torch\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "# 1) Prompt h·∫°t gi·ªëng (r·∫•t chung, d√πng ƒë∆∞·ª£c cho cartoon & real)\n",
    "SEED_PROMPTS = [\n",
    "    \"object\", \"person\", \"animal\",\n",
    "    \"cartoon character\", \"animated character\", \"toy\",\n",
    "    \"electronic device\", \"furniture\", \"vehicle\"\n",
    "]\n",
    "\n",
    "# T·ª´ ƒëi·ªÉn nh·ªè, ƒë·ªß x√†i cho cartoon ph·ªï th√¥ng\n",
    "COLORS  = {\"red\",\"pink\",\"blue\",\"green\",\"yellow\",\"purple\",\"orange\",\"black\",\"white\",\"gray\",\"brown\"}\n",
    "ANIMALS = {\"bear\",\"rabbit\",\"mouse\",\"cat\",\"dog\",\"moose\",\"beaver\",\"squirrel\",\"porcupine\",\"skunk\",\"fox\",\"deer\",\"bird\"}\n",
    "\n",
    "def gen_caption_prompts(image_pil, max_new_tokens=25):\n",
    "    \"\"\"D√πng BLIP ƒë·ªÉ sinh caption r·ªìi chuy·ªÉn th√†nh c√°c prompt ng·∫Øn, t·ªïng qu√°t.\"\"\"\n",
    "    try:\n",
    "        cap_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "        cap_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\").eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        inputs = cap_processor(image_pil, return_tensors=\"pt\").to(cap_model.device)\n",
    "        out = cap_model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "        caption = cap_processor.decode(out[0], skip_special_tokens=True).lower()\n",
    "    except Exception:\n",
    "        caption = \"\"  # n·∫øu kh√¥ng c√≥ internet / model, v·∫´n ch·∫°y ƒë∆∞·ª£c v·ªõi seed\n",
    "\n",
    "    # R√∫t tr√≠ch ƒë∆°n gi·∫£n: m√†u + danh t·ª´ kh·∫£ dƒ©\n",
    "    words = re.findall(r\"[a-z]+\", caption)\n",
    "    found_colors  = [w for w in words if w in COLORS]\n",
    "    found_animals = [w for w in words if w in ANIMALS]\n",
    "    # M·ªôt s·ªë danh m·ª•c t·ªïng qu√°t d·ªÖ g·∫∑p\n",
    "    GENERICS = {\"remote\", \"remote control\", \"tv\", \"screen\", \"phone\", \"laptop\",\n",
    "                \"chair\", \"table\", \"bed\", \"sofa\", \"book\", \"clock\", \"bag\", \"hat\", \"bow\"}\n",
    "\n",
    "    found_generics = [w for w in GENERICS if w in caption]\n",
    "\n",
    "    prompts = set()\n",
    "    # m√†u + lo√†i: \"blue rabbit\", \"pink bear\"\n",
    "    for c in found_colors:\n",
    "        for a in found_animals or [\"character\", \"animal\"]:\n",
    "            prompts.add(f\"{c} {a}\")\n",
    "            prompts.add(f\"{c} cartoon {a}\")\n",
    "            prompts.add(f\"{c} animated {a}\")\n",
    "\n",
    "    # c√°c danh m·ª•c t·ªïng qu√°t ph√°t hi·ªán t·ª´ caption\n",
    "    for g in found_generics:\n",
    "        prompts.add(g)\n",
    "        prompts.add(f\"cartoon {g}\")\n",
    "        prompts.add(f\"animated {g}\")\n",
    "\n",
    "    # n·∫øu caption n√≥i ƒë·∫øn ‚Äúcartoon/animation‚Äù, b·ªï sung v√†i c·ª•m r·ªông\n",
    "    if \"cartoon\" in caption or \"animation\" in caption or \"animated\" in caption:\n",
    "        prompts.update({\"cartoon animal\", \"cartoon face\", \"animated character\", \"mascot\", \"anthropomorphic animal\"})\n",
    "\n",
    "    # fallback: n·∫øu r·ªóng, v·∫´n tr·∫£ v·ªÅ v√†i c·ª•m chung\n",
    "    if not prompts:\n",
    "        prompts.update({\"cartoon character\", \"animated character\", \"animal\", \"object\"})\n",
    "\n",
    "    # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng ƒë·ªÉ tr√°nh nhi·ªÖu\n",
    "    prompts = list(prompts)[:20]\n",
    "    return prompts, caption\n",
    "\n",
    "def detect_general(image_pil, processor, model, box_th=0.5, text_th=0.4, device=\"cuda\"):\n",
    "    # 2) T·∫°o prompt t·ªïng: seed + auto-expansion\n",
    "    auto_prompts, caption = gen_caption_prompts(image_pil)\n",
    "    phrases = list(dict.fromkeys(SEED_PROMPTS + auto_prompts))[:32]  # unique + cap length\n",
    "\n",
    "    # 3) Grounding DINO inference\n",
    "    image_np = np.array(image_pil.convert(\"RGB\"))\n",
    "    inputs = processor(images=image_np, text=[phrases], return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    results = processor.post_process_grounded_object_detection(\n",
    "        outputs,\n",
    "        threshold=0.25,\n",
    "        target_sizes=[image_pil.size[::-1]]\n",
    "    )[0]\n",
    "\n",
    "    # tr·∫£ v·ªÅ c·∫£ caption ƒë·ªÉ debug\n",
    "    return phrases, caption, results\n",
    "\n",
    "# ----------------- USE -----------------\n",
    "# image = Image.open(\"/kaggle/input/cartoon/data/frames/scene_11_frame_783.jpg\").convert(\"RGB\")\n",
    "# phrases, caption, result = detect_general(image, processor, model, box_th=0.35, text_th=0.25, device=device)\n",
    "# print(\"caption:\", caption)\n",
    "# print(\"phrases:\", phrases)\n",
    "# for box, score, tl in zip(result[\"boxes\"], result[\"scores\"], result[\"text_labels\"]):\n",
    "#     print(f\"{tl} {score:.2f} @ {[round(x,2) for x in box.tolist()]}\")\n",
    "\n",
    "# Load ·∫£nh\n",
    "image = Image.open(\"/kaggle/input/cartoon/data/frames/scene_3_frame_131.jpg\").convert(\"RGB\")\n",
    "\n",
    "# G·ªçi h√†m detect_general (t·ª± sinh prompt v√† detect)\n",
    "phrases, caption, result = detect_general(image, processor, model, box_th=0.3, text_th=0.25, device=device)\n",
    "\n",
    "# In ra caption auto\n",
    "print(\"BLIP caption:\", caption)\n",
    "print(\"Prompts d√πng:\", phrases)\n",
    "\n",
    "# In k·∫øt qu·∫£ detection\n",
    "for box, score, text_label in zip(result[\"boxes\"], result[\"scores\"], result[\"text_labels\"]):\n",
    "    box = [round(x, 2) for x in box.tolist()]\n",
    "    print(f\"Detected {text_label} with confidence {round(score.item(), 3)} at location {box}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T13:19:53.462403Z",
     "iopub.status.busy": "2025-08-24T13:19:53.462103Z",
     "iopub.status.idle": "2025-08-24T13:19:53.841815Z",
     "shell.execute_reply": "2025-08-24T13:19:53.840931Z",
     "shell.execute_reply.started": "2025-08-24T13:19:53.462376Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "ax.imshow(image)\n",
    "\n",
    "for box, score, text_label in zip(result[\"boxes\"], result[\"scores\"], result[\"text_labels\"]):\n",
    "    x1, y1, x2, y2 = box.tolist()\n",
    "    rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor=\"red\", facecolor=\"none\")\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x1, y1, f\"{text_label} {score:.2f}\", color=\"yellow\", fontsize=12, bbox=dict(facecolor=\"black\", alpha=0.5))\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T13:14:59.006579Z",
     "iopub.status.busy": "2025-08-24T13:14:59.006261Z",
     "iopub.status.idle": "2025-08-24T13:15:04.155456Z",
     "shell.execute_reply": "2025-08-24T13:15:04.154541Z",
     "shell.execute_reply.started": "2025-08-24T13:14:59.006555Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!gdown 1Y42K4JuC8L6y8YPOTiTdkudhgYaaDq3q -O AnimeRunSample.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T13:15:10.125129Z",
     "iopub.status.busy": "2025-08-24T13:15:10.124833Z",
     "iopub.status.idle": "2025-08-24T13:15:10.616268Z",
     "shell.execute_reply": "2025-08-24T13:15:10.615388Z",
     "shell.execute_reply.started": "2025-08-24T13:15:10.125109Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!unzip AnimeRunSample.zip -d AnimeRunSample/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T13:47:18.064455Z",
     "iopub.status.busy": "2025-08-24T13:47:18.064147Z",
     "iopub.status.idle": "2025-08-24T13:55:13.586279Z",
     "shell.execute_reply": "2025-08-24T13:55:13.585278Z",
     "shell.execute_reply.started": "2025-08-24T13:47:18.064436Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import zipfile\n",
    "\n",
    "# --- Th∆∞ m·ª•c input/output ---\n",
    "input_dir = \"/kaggle/working/AnimeRunSample/AnimeRunSample\"\n",
    "output_dir = \"/kaggle/working/Process/AnimeRunSample\"\n",
    "zip_path = \"/kaggle/working/Process/AnimeRunSample.zip\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# --- L·∫∑p qua t·∫•t c·∫£ ·∫£nh trong th∆∞ m·ª•c ---\n",
    "for filename in os.listdir(input_dir):\n",
    "    if not filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        continue\n",
    "\n",
    "    img_path = os.path.join(input_dir, filename)\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    # G·ªçi detect_general cho t·ª´ng ·∫£nh\n",
    "    phrases, caption, result = detect_general(\n",
    "        image, processor, model,\n",
    "        box_th=0.5, text_th=0.4, device=device\n",
    "    )\n",
    "\n",
    "    # V·∫Ω box\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "    ax.imshow(image)\n",
    "\n",
    "    for box, score, text_label in zip(result[\"boxes\"], result[\"scores\"], result[\"text_labels\"]):\n",
    "        x1, y1, x2, y2 = box.tolist()\n",
    "        rect = patches.Rectangle(\n",
    "            (x1, y1), x2 - x1, y2 - y1,\n",
    "            linewidth=2, edgecolor=\"red\", facecolor=\"none\"\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(\n",
    "            x1, y1,\n",
    "            f\"{text_label} {score:.2f}\",\n",
    "            color=\"yellow\", fontsize=12,\n",
    "            bbox=dict(facecolor=\"black\", alpha=0.5)\n",
    "        )\n",
    "\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    # L∆∞u ·∫£nh k·∫øt qu·∫£\n",
    "    out_path = os.path.join(output_dir, filename)\n",
    "    plt.savefig(out_path, bbox_inches=\"tight\", pad_inches=0)\n",
    "    plt.close(fig)\n",
    "\n",
    "# --- N√©n to√†n b·ªô th∆∞ m·ª•c output th√†nh file zip ---\n",
    "with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for root, _, files in os.walk(output_dir):\n",
    "        for file in files:\n",
    "            filepath = os.path.join(root, file)\n",
    "            arcname = os.path.relpath(filepath, output_dir)\n",
    "            zipf.write(filepath, arcname)\n",
    "\n",
    "print(\"‚úÖ Ho√†n t·∫•t! File zip ·ªü:\", zip_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T13:18:17.840759Z",
     "iopub.status.busy": "2025-08-24T13:18:17.840431Z",
     "iopub.status.idle": "2025-08-24T13:18:18.210917Z",
     "shell.execute_reply": "2025-08-24T13:18:18.209868Z",
     "shell.execute_reply.started": "2025-08-24T13:18:17.840736Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "img = mpimg.imread(\"/kaggle/working/AnimeRunSample/AnimeRunSample/scene_10_frame_777.jpg\")\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")  # ·∫©n tr·ª•c\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T14:20:00.499885Z",
     "iopub.status.busy": "2025-08-24T14:20:00.499502Z",
     "iopub.status.idle": "2025-08-24T15:41:02.480023Z",
     "shell.execute_reply": "2025-08-24T15:41:02.479083Z",
     "shell.execute_reply.started": "2025-08-24T14:20:00.499859Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gdown\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import zipfile\n",
    "from tqdm import tqdm  # ƒë·ªÉ hi·ªÉn th·ªã progress bar\n",
    "\n",
    "# --- 1. T·∫£i video t·ª´ Google Drive ---\n",
    "drive_url = \"https://drive.google.com/file/d/1Ww6HASO9rOdn9N6dawv2-DNTRijpLkzE/view?usp=drive_link\"\n",
    "video_path = \"/kaggle/working/input_video.mp4\"\n",
    "\n",
    "# T·∫£i file (fuzzy=True ƒë·ªÉ t·ª± ƒë·ªông nh·∫≠n ID t·ª´ link view)\n",
    "gdown.download(url=drive_url, output=video_path, fuzzy=True, quiet=False)\n",
    "\n",
    "# --- 2. Chu·∫©n b·ªã th∆∞ m·ª•c l∆∞u frames ---\n",
    "frames_dir = \"/kaggle/working/processed_frames\"\n",
    "os.makedirs(frames_dir, exist_ok=True)\n",
    "\n",
    "# M·ªü video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "frame_idx = 0\n",
    "\n",
    "# --- 3. X·ª≠ l√Ω t·ª´ng frame ---\n",
    "with tqdm(total=total_frames, desc=\"Processing frames\") as pbar:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Chuy·ªÉn BGR (OpenCV) -> RGB (PIL)\n",
    "        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        # üîπ G·ªçi detect_general (b·∫°n ph·∫£i c√≥ s·∫µn processor, model, device)\n",
    "        phrases, caption, result = detect_general(\n",
    "            image, processor, model, box_th=0.3, text_th=0.25, device=device\n",
    "        )\n",
    "\n",
    "        # V·∫Ω bounding box v·ªõi Matplotlib\n",
    "        fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "        ax.imshow(image)\n",
    "        for box, score, text_label in zip(result[\"boxes\"], result[\"scores\"], result[\"text_labels\"]):\n",
    "            x1, y1, x2, y2 = box.tolist()\n",
    "            rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                                     linewidth=2, edgecolor=\"red\", facecolor=\"none\")\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(x1, y1, f\"{text_label} {score:.2f}\",\n",
    "                    color=\"yellow\", fontsize=12,\n",
    "                    bbox=dict(facecolor=\"black\", alpha=0.5))\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        out_frame_path = os.path.join(frames_dir, f\"frame_{frame_idx:05d}.jpg\")\n",
    "        plt.savefig(out_frame_path, bbox_inches=\"tight\", pad_inches=0)\n",
    "        plt.close(fig)\n",
    "\n",
    "        frame_idx += 1\n",
    "        pbar.update(1)\n",
    "\n",
    "cap.release()\n",
    "\n",
    "# --- 4. Xu·∫•t video ƒë·∫ßu ra t·ª´ frames ---\n",
    "output_video = \"/kaggle/working/output_video.mp4\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))\n",
    "\n",
    "for i in range(frame_idx):\n",
    "    frame_path = os.path.join(frames_dir, f\"frame_{i:05d}.jpg\")\n",
    "    img = cv2.imread(frame_path)\n",
    "    out.write(img)\n",
    "\n",
    "out.release()\n",
    "\n",
    "# --- 5. N√©n th∆∞ m·ª•c frame th√†nh ZIP ---\n",
    "zip_path = \"/kaggle/working/processed_frames.zip\"\n",
    "with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for root, _, files in os.walk(frames_dir):\n",
    "        for file in files:\n",
    "            filepath = os.path.join(root, file)\n",
    "            arcname = os.path.relpath(filepath, frames_dir)\n",
    "            zipf.write(filepath, arcname)\n",
    "\n",
    "print(\"‚úÖ Ho√†n t·∫•t!\")\n",
    "print(\"Video ƒë·∫ßu ra:\", output_video)\n",
    "print(\"ZIP frames:\", zip_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T13:09:56.847554Z",
     "iopub.status.busy": "2025-08-24T13:09:56.847008Z",
     "iopub.status.idle": "2025-08-24T13:09:59.050735Z",
     "shell.execute_reply": "2025-08-24T13:09:59.049754Z",
     "shell.execute_reply.started": "2025-08-24T13:09:56.847534Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "frames_dir = \"/kaggle/working/processed_frames\"\n",
    "# --- 5. N√©n th∆∞ m·ª•c frame th√†nh ZIP ---\n",
    "zip_path = \"/kaggle/working/processed_frames.zip\"\n",
    "with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for root, _, files in os.walk(frames_dir):\n",
    "        for file in files:\n",
    "            filepath = os.path.join(root, file)\n",
    "            arcname = os.path.relpath(filepath, frames_dir)\n",
    "            zipf.write(filepath, arcname)\n",
    "\n",
    "print(\"‚úÖ Ho√†n t·∫•t!\")\n",
    "#print(\"Video ƒë·∫ßu ra:\", output_video)\n",
    "print(\"ZIP frames:\", zip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDEA-Research/grounding-dino-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T13:09:59.052521Z",
     "iopub.status.busy": "2025-08-24T13:09:59.051738Z",
     "iopub.status.idle": "2025-08-24T13:10:06.412180Z",
     "shell.execute_reply": "2025-08-24T13:10:06.411244Z",
     "shell.execute_reply.started": "2025-08-24T13:09:59.052500Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"import requests\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "\n",
    "model_id = \"IDEA-Research/grounding-dino-base\"\n",
    "device = \"cuda\"\n",
    "\n",
    "processor2 = AutoProcessor.from_pretrained(model_id)\n",
    "model2 = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T13:10:06.413774Z",
     "iopub.status.busy": "2025-08-24T13:10:06.413199Z",
     "iopub.status.idle": "2025-08-24T13:10:09.499822Z",
     "shell.execute_reply": "2025-08-24T13:10:09.498484Z",
     "shell.execute_reply.started": "2025-08-24T13:10:06.413746Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Check for cats and remote controls\n",
    "import re, numpy as np, torch\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "# 1) Prompt h·∫°t gi·ªëng (r·∫•t chung, d√πng ƒë∆∞·ª£c cho cartoon & real)\n",
    "SEED_PROMPTS = [\n",
    "    \"object\", \"person\", \"animal\",\n",
    "    \"cartoon character\", \"animated character\", \"toy\",\n",
    "    \"electronic device\", \"furniture\", \"vehicle\"\n",
    "]\n",
    "\n",
    "# T·ª´ ƒëi·ªÉn nh·ªè, ƒë·ªß x√†i cho cartoon ph·ªï th√¥ng\n",
    "COLORS  = {\"red\",\"pink\",\"blue\",\"green\",\"yellow\",\"purple\",\"orange\",\"black\",\"white\",\"gray\",\"brown\"}\n",
    "ANIMALS = {\"bear\",\"rabbit\",\"mouse\",\"cat\",\"dog\",\"moose\",\"beaver\",\"squirrel\",\"porcupine\",\"skunk\",\"fox\",\"deer\",\"bird\"}\n",
    "\n",
    "def gen_caption_prompts(image_pil, max_new_tokens=25):\n",
    "    \"\"\"D√πng BLIP ƒë·ªÉ sinh caption r·ªìi chuy·ªÉn th√†nh c√°c prompt ng·∫Øn, t·ªïng qu√°t.\"\"\"\n",
    "    try:\n",
    "        cap_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "        cap_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\").eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        inputs = cap_processor(image_pil, return_tensors=\"pt\").to(cap_model.device)\n",
    "        out = cap_model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "        caption = cap_processor.decode(out[0], skip_special_tokens=True).lower()\n",
    "    except Exception:\n",
    "        caption = \"\"  # n·∫øu kh√¥ng c√≥ internet / model, v·∫´n ch·∫°y ƒë∆∞·ª£c v·ªõi seed\n",
    "\n",
    "    # R√∫t tr√≠ch ƒë∆°n gi·∫£n: m√†u + danh t·ª´ kh·∫£ dƒ©\n",
    "    words = re.findall(r\"[a-z]+\", caption)\n",
    "    found_colors  = [w for w in words if w in COLORS]\n",
    "    found_animals = [w for w in words if w in ANIMALS]\n",
    "    # M·ªôt s·ªë danh m·ª•c t·ªïng qu√°t d·ªÖ g·∫∑p\n",
    "    GENERICS = {\"remote\", \"remote control\", \"tv\", \"screen\", \"phone\", \"laptop\",\n",
    "                \"chair\", \"table\", \"bed\", \"sofa\", \"book\", \"clock\", \"bag\", \"hat\", \"bow\"}\n",
    "\n",
    "    found_generics = [w for w in GENERICS if w in caption]\n",
    "\n",
    "    prompts = set()\n",
    "    # m√†u + lo√†i: \"blue rabbit\", \"pink bear\"\n",
    "    for c in found_colors:\n",
    "        for a in found_animals or [\"character\", \"animal\"]:\n",
    "            prompts.add(f\"{c} {a}\")\n",
    "            prompts.add(f\"{c} cartoon {a}\")\n",
    "            prompts.add(f\"{c} animated {a}\")\n",
    "\n",
    "    # c√°c danh m·ª•c t·ªïng qu√°t ph√°t hi·ªán t·ª´ caption\n",
    "    for g in found_generics:\n",
    "        prompts.add(g)\n",
    "        prompts.add(f\"cartoon {g}\")\n",
    "        prompts.add(f\"animated {g}\")\n",
    "\n",
    "    # n·∫øu caption n√≥i ƒë·∫øn ‚Äúcartoon/animation‚Äù, b·ªï sung v√†i c·ª•m r·ªông\n",
    "    if \"cartoon\" in caption or \"animation\" in caption or \"animated\" in caption:\n",
    "        prompts.update({\"cartoon animal\", \"cartoon face\", \"animated character\", \"mascot\", \"anthropomorphic animal\"})\n",
    "\n",
    "    # fallback: n·∫øu r·ªóng, v·∫´n tr·∫£ v·ªÅ v√†i c·ª•m chung\n",
    "    if not prompts:\n",
    "        prompts.update({\"cartoon character\", \"animated character\", \"animal\", \"object\"})\n",
    "\n",
    "    # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng ƒë·ªÉ tr√°nh nhi·ªÖu\n",
    "    prompts = list(prompts)[:20]\n",
    "    return prompts, caption\n",
    "\n",
    "def detect_general(image_pil, processor, model, box_th=0.35, text_th=0.25, device=\"cuda\"):\n",
    "    # 2) T·∫°o prompt t·ªïng: seed + auto-expansion\n",
    "    auto_prompts, caption = gen_caption_prompts(image_pil)\n",
    "    phrases = list(dict.fromkeys(SEED_PROMPTS + auto_prompts))[:32]  # unique + cap length\n",
    "\n",
    "    # 3) Grounding DINO inference\n",
    "    image_np = np.array(image_pil.convert(\"RGB\"))\n",
    "    inputs = processor(images=image_np, text=[phrases], return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    results = processor.post_process_grounded_object_detection(\n",
    "        outputs,\n",
    "        inputs.input_ids,\n",
    "        box_threshold=box_th,\n",
    "        text_threshold=text_th,\n",
    "        target_sizes=[image_pil.size[::-1]]\n",
    "    )[0]\n",
    "\n",
    "    # tr·∫£ v·ªÅ c·∫£ caption ƒë·ªÉ debug\n",
    "    return phrases, caption, results\n",
    "\n",
    "# ----------------- USE -----------------\n",
    "# image = Image.open(\"/kaggle/input/cartoon/data/frames/scene_11_frame_783.jpg\").convert(\"RGB\")\n",
    "# phrases, caption, result = detect_general(image, processor, model, box_th=0.35, text_th=0.25, device=device)\n",
    "# print(\"caption:\", caption)\n",
    "# print(\"phrases:\", phrases)\n",
    "# for box, score, tl in zip(result[\"boxes\"], result[\"scores\"], result[\"text_labels\"]):\n",
    "#     print(f\"{tl} {score:.2f} @ {[round(x,2) for x in box.tolist()]}\")\n",
    "\n",
    "# Load ·∫£nh\n",
    "image2 = Image.open(\"/kaggle/input/cartoon/data/original/Image0182.png\").convert(\"RGB\")\n",
    "\n",
    "# G·ªçi h√†m detect_general (t·ª± sinh prompt v√† detect)\n",
    "phrases2, caption2, result2 = detect_general(image2, processor2, model2, box_th=0.3, text_th=0.25, device=device)\n",
    "\n",
    "# In ra caption auto\n",
    "print(\"BLIP caption:\", caption2)\n",
    "print(\"Prompts d√πng:\", phrases2)\n",
    "\n",
    "# In k·∫øt qu·∫£ detection\n",
    "for box, score, text_label in zip(result2[\"boxes\"], result2[\"scores\"], result2[\"text_labels\"]):\n",
    "    box = [round(x, 2) for x in box.tolist()]\n",
    "    print(f\"Detected {text_label} with confidence {round(score.item(), 3)} at location {box}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-24T13:10:09.500447Z",
     "iopub.status.idle": "2025-08-24T13:10:09.500719Z",
     "shell.execute_reply": "2025-08-24T13:10:09.500598Z",
     "shell.execute_reply.started": "2025-08-24T13:10:09.500586Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "ax.imshow(image2)\n",
    "\n",
    "for box, score, text_label in zip(result2[\"boxes\"], result2[\"scores\"], result2[\"text_labels\"]):\n",
    "    x1, y1, x2, y2 = box.tolist()\n",
    "    rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor=\"red\", facecolor=\"none\")\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x1, y1, f\"{text_label} {score:.2f}\", color=\"yellow\", fontsize=12, bbox=dict(facecolor=\"black\", alpha=0.5))\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Story Coherence Evaluation\n",
    "\n",
    "## √ù t∆∞·ªüng\n",
    "\n",
    "S·ª≠ d·ª•ng **BLIP** ƒë·ªÉ ƒë√°nh gi√° ƒë·ªô ph√π h·ª£p v·ªÅ m·∫∑t c√¢u chuy·ªán gi·ªØa:\n",
    "1. **·∫¢nh g·ªëc (Full Image)** - B·ªëi c·∫£nh t·ªïng th·ªÉ c·ªßa c√¢u chuy·ªán\n",
    "2. **V√πng crop (t·ª´ Object Detection)** - C√°c chi ti·∫øt c·ª•c b·ªô\n",
    "\n",
    "### Pipeline\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  ·∫¢nh g·ªëc    ‚îÇ\n",
    "‚îÇ  (Full)     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ\n",
    "       ‚îú‚îÄ‚îÄ‚ñ∫ BLIP Caption ‚îÄ‚îÄ‚ñ∫ \"a cartoon character running in the forest\"\n",
    "       ‚îÇ                     (B·ªëi c·∫£nh t·ªïng th·ªÉ)\n",
    "       ‚îÇ\n",
    "       ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Object Detection    ‚îÇ\n",
    "‚îÇ (Grounding DINO)    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ\n",
    "       ‚îú‚îÄ‚îÄ‚ñ∫ Crop 1: Character face\n",
    "       ‚îú‚îÄ‚îÄ‚ñ∫ Crop 2: Background tree\n",
    "       ‚îî‚îÄ‚îÄ‚ñ∫ Crop 3: Running shoes\n",
    "       \n",
    "       ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ BLIP Caption m·ªói    ‚îÇ\n",
    "‚îÇ v√πng crop           ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ\n",
    "       ‚îú‚îÄ‚îÄ‚ñ∫ Crop 1: \"a smiling cartoon character\"\n",
    "       ‚îú‚îÄ‚îÄ‚ñ∫ Crop 2: \"green trees in forest\"  \n",
    "       ‚îî‚îÄ‚îÄ‚ñ∫ Crop 3: \"red and white shoes\"\n",
    "       \n",
    "       ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Semantic Similarity ‚îÇ\n",
    "‚îÇ (Sentence-BERT)     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ\n",
    "       ‚îî‚îÄ‚îÄ‚ñ∫ Scores:\n",
    "            - Crop 1: 0.85 ‚úì (li√™n quan cao)\n",
    "            - Crop 2: 0.72 ‚úì (li√™n quan trung b√¨nh)\n",
    "            - Crop 3: 0.45 ‚úó (√≠t li√™n quan)\n",
    "```\n",
    "\n",
    "## M·ª•c ƒë√≠ch\n",
    "\n",
    "### 1. **L·ªçc v√πng quan tr·ªçng**\n",
    "- V√πng c√≥ **similarity cao** ‚Üí gi·ªØ ƒë∆∞·ª£c b·ªëi c·∫£nh c√¢u chuy·ªán\n",
    "- V√πng c√≥ **similarity th·∫•p** ‚Üí chi ti·∫øt kh√¥ng quan tr·ªçng\n",
    "\n",
    "### 2. **ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng crop**\n",
    "- Caption c·ªßa crop c√≥ \"k·ªÉ ƒë∆∞·ª£c c√¢u chuy·ªán\" kh√¥ng?\n",
    "- Crop c√≥ capture ƒë∆∞·ª£c √Ω nghƒ©a ch√≠nh kh√¥ng?\n",
    "\n",
    "### 3. **Ranking crops**\n",
    "S·∫Øp x·∫øp c√°c v√πng theo ƒë·ªô quan tr·ªçng:\n",
    "- **Story Similarity** (m·ªõi): Ph√π h·ª£p v·ªÅ c√¢u chuy·ªán\n",
    "- **Detection Score**: ƒê·ªô tin c·∫≠y c·ªßa model\n",
    "- **Edge Density**: ƒê·ªô ph·ª©c t·∫°p n√©t v·∫Ω\n",
    "- **Entropy**: ƒê·ªô ph·ª©c t·∫°p th√¥ng tin\n",
    "\n",
    "## V√≠ d·ª• th·ª±c t·∫ø\n",
    "\n",
    "**·∫¢nh g·ªëc**: \"Two friends playing basketball in the park\"\n",
    "\n",
    "| Crop | Caption | Similarity | ƒê√°nh gi√° |\n",
    "|------|---------|-----------|----------|\n",
    "| 1 | \"A person holding a basketball\" | 0.89 | ‚úì Quan tr·ªçng |\n",
    "| 2 | \"Green grass and trees\" | 0.65 | ~ Ph·ª• |\n",
    "| 3 | \"Blue sky with clouds\" | 0.42 | ‚úó √çt li√™n quan |\n",
    "\n",
    "‚Üí **Crop 1** n√™n ƒë∆∞·ª£c ∆∞u ti√™n v√¨ gi·ªØ ƒë∆∞·ª£c c√¢u chuy·ªán ch√≠nh!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required package\n",
    "!pip install sentence-transformers\n",
    "\n",
    "# Import\n",
    "import sys\n",
    "sys.path.append('/home/serverai/ltdoanh/LayoutGeneration')\n",
    "from objectfree.story_coherence_evaluator import StoryCoherenceEvaluator\n",
    "\n",
    "# Initialize\n",
    "evaluator = StoryCoherenceEvaluator(\n",
    "    blip_model_name=\"Salesforce/blip-image-captioning-large\",\n",
    "    device=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test v·ªõi m·ªôt ·∫£nh ƒë∆°n\n",
    "test_image = \"/kaggle/input/keyframes/14653_keyframes/0002_clip11_frame071_14653_11.jpg\"\n",
    "\n",
    "# Gi·∫£ s·ª≠ ƒë√£ c√≥ detection results\n",
    "test_detections = [\n",
    "    {'bbox': [100, 100, 300, 300], 'score': 0.9, 'label': 'character'},\n",
    "    {'bbox': [400, 200, 600, 400], 'score': 0.8, 'label': 'background'},\n",
    "    {'bbox': [50, 50, 150, 150], 'score': 0.7, 'label': 'detail'}\n",
    "]\n",
    "\n",
    "# Ch·∫°y evaluation\n",
    "result = evaluator.evaluate_single_image(\n",
    "    image_path=test_image,\n",
    "    detections=test_detections,\n",
    "    output_dir=\"/kaggle/working/story_eval\",\n",
    "    save_crops=True\n",
    ")\n",
    "\n",
    "# Hi·ªÉn th·ªã k·∫øt qu·∫£\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Full Image Caption: {result['full_caption']}\")\n",
    "print(f\"\\nNumber of crops: {result['num_valid_crops']}\")\n",
    "print(f\"Average similarity: {result['avg_similarity']:.3f}\")\n",
    "print(f\"Max similarity: {result['max_similarity']:.3f}\")\n",
    "print(f\"Min similarity: {result['min_similarity']:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"TOP 3 CROPS (ranked by story similarity)\")\n",
    "print(\"-\"*70)\n",
    "for i, crop in enumerate(result['crops'][:3]):\n",
    "    print(f\"\\n{i+1}. Crop {crop['crop_id']}\")\n",
    "    print(f\"   Caption: \\\"{crop['crop_caption']}\\\"\")\n",
    "    print(f\"   Story Similarity: {crop['story_similarity']:.3f}\")\n",
    "    print(f\"   Detection Score: {crop['detection_score']:.3f}\")\n",
    "    print(f\"   Label: {crop['detection_label']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4553891,
     "sourceId": 7781776,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4554482,
     "sourceId": 7782583,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8050118,
     "sourceId": 12735468,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8358629,
     "sourceId": 13189799,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8358661,
     "sourceId": 13189846,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
